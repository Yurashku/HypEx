import numpy as np
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, KBinsDiscretizer
from sklearn.compose import ColumnTransformer
from catboost import CatBoostRegressor
import joblib
import os
import argparse
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

from custom_elements import (
    RatioFeatureGenerator, 
    LogFeatureGenerator,
    IQROutlierHandler, 
    ZScoreOutlierHandler, 
    PercentileOutlierHandler,
    ResultSaver
)


def load_california_housing():
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ California Housing –∏–∑ sklearn.
    
    Returns:
        tuple: (features_df, target_series, feature_names, description)
    """
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ California Housing...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    housing = fetch_california_housing(as_frame=True)
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    features_df = housing.data
    target_series = housing.target
    feature_names = housing.feature_names
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã:")
    print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤: {features_df.shape[0]:,}")
    print(f"   ‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_df.shape[1]}")
    print(f"   ‚Ä¢ –ü—Ä–∏–∑–Ω–∞–∫–∏: {feature_names}")
    print(f"   ‚Ä¢ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: —Å—Ç–æ–∏–º–æ—Å—Ç—å –∂–∏–ª—å—è (—Å–æ—Ç–Ω–∏ —Ç—ã—Å—è—á –¥–æ–ª–ª–∞—Ä–æ–≤)")
    print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω target: {target_series.min():.2f} - {target_series.max():.2f}")
    
    return features_df, target_series, feature_names, housing.DESCR


def create_feature_engineering_pipeline():
    """
    –°–æ–∑–¥–∞–Ω–∏–µ pipeline –¥–ª—è feature engineering —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º 
    —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∏ sklearn.
    """
    print("\nüé® –°–æ–∑–¥–∞–Ω–∏–µ Feature Engineering Pipeline...")
    
    # –°–æ–∑–¥–∞–µ–º FeatureUnion –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_engineering = FeatureUnion([
        # 1. –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø—Ä–æ—Ö–æ–¥—è—Ç –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
        ('original', 'passthrough'),
        
        # 2. –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (sklearn)
        ('polynomial', PolynomialFeatures(
            degree=2, 
            include_bias=False, 
            interaction_only=False
        )),
        
        # 3. –¢–æ–ª—å–∫–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (sklearn)  
        ('interactions', PolynomialFeatures(
            degree=2,
            include_bias=False,
            interaction_only=True
        )),
        
        # 4. –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–∫–∞—Å—Ç–æ–º–Ω—ã–π)
        ('ratios', RatioFeatureGenerator(
            max_features=8,
            min_std_threshold=0.01
        )),
        
        # 5. –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–∞—Å—Ç–æ–º–Ω—ã–π)
        ('logarithmic', LogFeatureGenerator(
            min_positive_ratio=0.8
        )),
        
        # 6. –ë–∏–Ω–Ω–∏–Ω–≥ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (sklearn)
        ('binning', KBinsDiscretizer(
            n_bins=5,
            encode='ordinal',
            strategy='quantile'
        ))
    ])
    
    print("‚úÖ Feature Engineering —Å–æ–∑–¥–∞–Ω —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏:")
    for name, transformer in feature_engineering.transformer_list:
        if hasattr(transformer, '__class__'):
            print(f"   ‚Ä¢ {name}: {transformer.__class__.__name__}")
        else:
            print(f"   ‚Ä¢ {name}: {transformer}")
    
    return feature_engineering


def create_ml_pipeline(outlier_method='iqr', random_state=42):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ ML pipeline —Å –≤—ã–±–∏—Ä–∞–µ–º—ã–º –º–µ—Ç–æ–¥–æ–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤.
    
    Args:
        outlier_method: 'iqr', 'zscore', –∏–ª–∏ 'percentile'
    """
    print(f"\nüîß –°–æ–∑–¥–∞–Ω–∏–µ ML Pipeline —Å {outlier_method.upper()} outlier handler...")
    
    # –í—ã–±–æ—Ä –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
    if outlier_method == 'iqr':
        outlier_handler = IQROutlierHandler(factor=1.5)
    elif outlier_method == 'zscore':
        outlier_handler = ZScoreOutlierHandler(threshold=3.0)
    elif outlier_method == 'percentile':
        outlier_handler = PercentileOutlierHandler(
            lower_percentile=1.0, 
            upper_percentile=99.0
        )
    else:
        raise ValueError(f"Unknown outlier method: {outlier_method}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ feature engineering pipeline
    feature_engineering = create_feature_engineering_pipeline()
    
    # –ü–æ–ª–Ω—ã–π pipeline
    pipeline = Pipeline([
        # 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤
        ('outlier_handler', outlier_handler),
        
        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
        ('feature_engineering', feature_engineering),
        
        # 3. –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        ('scaler', StandardScaler()),
        
        # 4. CatBoost —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä
        ('regressor', CatBoostRegressor(
            iterations=500,
            learning_rate=0.1,
            depth=6,
            l2_leaf_reg=3,
            random_seed=random_state,
            verbose=False,
            allow_writing_files=False
        ))
    ])
    
    print("‚úÖ Pipeline —Å–æ–∑–¥–∞–Ω —Å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏:")
    for step_name, step_estimator in pipeline.steps:
        print(f"   ‚Ä¢ {step_name}: {step_estimator.__class__.__name__}")
    
    return pipeline


def save_pipeline(pipeline, filepath, additional_info=None):
    """
    –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline –≤ —Ñ–∞–π–ª.
    
    Args:
        pipeline: –æ–±—É—á–µ–Ω–Ω—ã–π sklearn Pipeline
        filepath: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        additional_info: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    """
    print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
    pipeline_data = {
        'pipeline': pipeline,
        'save_timestamp': pd.Timestamp.now().isoformat(),
        'sklearn_version': joblib.__version__,
        'additional_info': additional_info or {}
    }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø–æ–º–æ—â—å—é joblib (–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –¥–ª—è sklearn –æ–±—ä–µ–∫—Ç–æ–≤)
    joblib.dump(pipeline_data, filepath, compress=3)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
    file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB
    
    print(f"‚úÖ Pipeline —Å–æ—Ö—Ä–∞–Ω–µ–Ω:")
    print(f"   ‚Ä¢ –§–∞–π–ª: {filepath}")
    print(f"   ‚Ä¢ –†–∞–∑–º–µ—Ä: {file_size:.2f} MB")
    print(f"   ‚Ä¢ –í—Ä–µ–º—è: {pipeline_data['save_timestamp']}")
    
    return filepath


def load_pipeline(filepath):
    """
    –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline –∏–∑ —Ñ–∞–π–ª–∞.
    
    Args:
        filepath: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å pipeline
        
    Returns:
        tuple: (pipeline, metadata)
    """
    print(f"\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline...")
    
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"Pipeline —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {filepath}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    pipeline_data = joblib.load(filepath)
    
    pipeline = pipeline_data['pipeline']
    metadata = {
        'save_timestamp': pipeline_data.get('save_timestamp', 'Unknown'),
        'sklearn_version': pipeline_data.get('sklearn_version', 'Unknown'),
        'additional_info': pipeline_data.get('additional_info', {})
    }
    
    print(f"‚úÖ Pipeline –∑–∞–≥—Ä—É–∂–µ–Ω:")
    print(f"   ‚Ä¢ –§–∞–π–ª: {filepath}")
    print(f"   ‚Ä¢ –°–æ—Ö—Ä–∞–Ω–µ–Ω: {metadata['save_timestamp']}")
    print(f"   ‚Ä¢ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã: {len(pipeline.steps)} —à–∞–≥–æ–≤")
    
    for step_name, step_estimator in pipeline.steps:
        print(f"     - {step_name}: {step_estimator.__class__.__name__}")
    
    return pipeline, metadata


def demonstrate_different_outlier_methods(features_train, target_train, features_test, target_test):
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤.
    """
    print(f"\nüî¨ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤:")
    print("=" * 50)
    
    methods = ['iqr', 'zscore', 'percentile']
    results = {}
    result_saver = ResultSaver()
    
    for method in methods:
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–µ—Ç–æ–¥: {method.upper()}")
        print("-" * 30)
        
        # –°–æ–∑–¥–∞–µ–º pipeline —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
        pipeline = create_ml_pipeline(outlier_method=method, random_state=42)
        
        # –û–±—É—á–µ–Ω–∏–µ
        print(f"   üéØ –û–±—É—á–µ–Ω–∏–µ —Å {method}...")
        pipeline.fit(features_train, target_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        train_pred = pipeline.predict(features_train)
        test_pred = pipeline.predict(features_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        train_metrics = result_saver.calculate_metrics(target_train, train_pred)
        test_metrics = result_saver.calculate_metrics(target_test, test_pred)
        
        results[method] = {
            'train_r2': train_metrics['r2'],
            'test_r2': test_metrics['r2'],
            'train_mape': train_metrics['mape'],
            'test_mape': test_metrics['mape'],
            'overfitting': train_metrics['r2'] - test_metrics['r2']
        }
        
        print(f"   üìä Test R¬≤: {test_metrics['r2']:.3f} | Test MAPE: {test_metrics['mape']:.3f}")
    
    # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    print(f"\nüìã –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –º–µ—Ç–æ–¥–æ–≤:")
    print("-" * 60)
    print(f"{'–ú–µ—Ç–æ–¥':<12} {'Test R¬≤':<10} {'Test MAPE':<12} {'–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ':<12}")
    print("-" * 60)
    
    for method, metrics in results.items():
        print(f"{method.upper():<12} {metrics['test_r2']:<10.3f} {metrics['test_mape']:<12.3f} {metrics['overfitting']:<12.3f}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–∏–π –º–µ—Ç–æ–¥
    best_method = max(results.items(), key=lambda x: x[1]['test_r2'])
    print(f"\nüèÜ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method[0].upper()} (Test R¬≤ = {best_method[1]['test_r2']:.3f})")
    
    return best_method[0], results


def train_phase(features_train, target_train, best_method, models_dir="models"):
    """
    –§–∞–∑–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º pipeline.
    
    Returns:
        tuple: (pipeline_filepath, train_predictions, data_info)
    """
    print(f"\nüéØ –§–ê–ó–ê –û–ë–£–ß–ï–ù–ò–Ø –Ω–∞ {features_train.shape[0]:,} –æ–±—ä–µ–∫—Ç–∞—Ö...")
    print("=" * 50)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ pipeline —Å –ª—É—á—à–∏–º –º–µ—Ç–æ–¥–æ–º
    pipeline = create_ml_pipeline(outlier_method=best_method, random_state=42)
    
    # –û–±—É—á–µ–Ω–∏–µ pipeline
    print(f"\nüöÄ –û–±—É—á–µ–Ω–∏–µ pipeline...")
    pipeline.fit(features_train, target_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    train_predictions = pipeline.predict(features_train)
    
    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π –ø–æ —ç—Ç–∞–ø–∞–º
    print(f"\nüîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–π:")
    
    # 1. –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤
    after_outliers = pipeline.named_steps['outlier_handler'].transform(features_train)
    print(f"   ‚Ä¢ –ü–æ—Å–ª–µ outlier handler: {after_outliers.shape}")
    
    # 2. –ü–æ—Å–ª–µ feature engineering
    after_features = pipeline.named_steps['feature_engineering'].transform(after_outliers)
    print(f"   ‚Ä¢ –ü–æ—Å–ª–µ feature engineering: {after_features.shape}")
    
    # 3. –ü–æ—Å–ª–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
    after_scaling = pipeline.named_steps['scaler'].transform(after_features)
    print(f"   ‚Ä¢ –ü–æ—Å–ª–µ scaling: {after_scaling.shape}")
    
    data_info = {
        'original_features': features_train.shape[1],
        'after_outlier_handling': after_outliers.shape[1],
        'after_feature_engineering': after_features.shape[1], 
        'final_features': after_scaling.shape[1],
        'train_samples': features_train.shape[0],
        'feature_expansion_ratio': after_features.shape[1] / features_train.shape[1],
        'best_outlier_method': best_method
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline
    timestamp = pd.Timestamp.now().strftime("%Y%m%d_%H%M%S")
    pipeline_filename = f"california_housing_pipeline_{best_method}_{timestamp}.joblib"
    pipeline_filepath = os.path.join(models_dir, pipeline_filename)
    
    additional_info = {
        'dataset': 'California Housing',
        'best_outlier_method': best_method,
        'data_info': data_info
    }
    
    saved_filepath = save_pipeline(pipeline, pipeline_filepath, additional_info)
    
    print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ:")
    print(f"   ‚Ä¢ –ò—Å—Ö–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {data_info['original_features']}")
    print(f"   ‚Ä¢ –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {data_info['final_features']}")
    print(f"   ‚Ä¢ –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ: {data_info['feature_expansion_ratio']:.1f}x")
    print(f"   ‚Ä¢ –õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_method.upper()}")
    print(f"   ‚Ä¢ Pipeline —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {saved_filepath}")
    
    return saved_filepath, train_predictions, data_info


def inference_phase(features_test, pipeline_filepath):
    """
    –§–∞–∑–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ pipeline.
    
    Args:
        features_test: —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        pipeline_filepath: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É pipeline
        
    Returns:
        tuple: (test_predictions, pipeline_metadata)
    """
    print(f"\nüîÆ –§–ê–ó–ê –ò–ù–§–ï–†–ï–ù–°–ê –Ω–∞ {features_test.shape[0]:,} –æ–±—ä–µ–∫—Ç–∞—Ö...")
    print("=" * 50)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ pipeline
    pipeline, metadata = load_pipeline(pipeline_filepath)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüöÄ –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    test_predictions = pipeline.predict(features_test)
    
    print(f"‚úÖ –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à–µ–Ω:")
    print(f"   ‚Ä¢ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ–ª—É—á–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö {len(test_predictions):,} –æ–±—ä–µ–∫—Ç–æ–≤")
    print(f"   ‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {test_predictions.min():.2f} - {test_predictions.max():.2f}")
    print(f"   ‚Ä¢ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞: {metadata['save_timestamp']}")
    
    return test_predictions, metadata


def analyze_and_save_results(target_train, train_pred, target_test, test_pred, 
                            data_info, best_method, result_saver):
    """
    –ê–Ω–∞–ª–∏–∑ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞.
    """
    print(f"\nüìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í...")
    print("=" * 50)
    
    # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    train_metrics = result_saver.calculate_metrics(target_train, train_pred)
    test_metrics = result_saver.calculate_metrics(target_test, test_pred)
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏
    model_info = {
        'model_type': 'CatBoostRegressor',
        'pipeline_components': [
            f'{best_method.upper()}OutlierHandler',
            'FeatureUnion[PolynomialFeatures, Interactions, Ratios, Log, Binning]',
            'StandardScaler', 
            'CatBoostRegressor'
        ],
        'feature_engineering': {
            'original_features': ['passthrough'],
            'sklearn_components': ['PolynomialFeatures', 'KBinsDiscretizer'],
            'custom_components': ['RatioFeatureGenerator', 'LogFeatureGenerator']
        },
        'outlier_method': best_method,
        'hyperparameters': {
            'iterations': 500,
            'learning_rate': 0.1,
            'depth': 6,
            'l2_leaf_reg': 3
        },
        'pipeline_saved': True
    }
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    additional_info = {
        'dataset': 'California Housing',
        'test_size': 0.2,
        'random_state': 42,
        'specialized_components': True,
        'outlier_methods_compared': ['iqr', 'zscore', 'percentile'],
        'workflow': 'separate_train_inference'
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    filepath = result_saver.save_experiment(
        experiment_name='california_separate_workflow',
        model_info=model_info,
        train_metrics=train_metrics,
        test_metrics=test_metrics,
        data_info=data_info,
        additional_info=additional_info
    )
    
    # –í—ã–≤–æ–¥ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
    print(f"\nüìà –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–º–µ—Ç–æ–¥: {best_method.upper()}):")
    print(f"   Train MAPE: {train_metrics['mape']:.3f} | Test MAPE: {test_metrics['mape']:.3f}")
    print(f"   Train R¬≤:   {train_metrics['r2']:.3f} | Test R¬≤:   {test_metrics['r2']:.3f}")
    print(f"   –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ R¬≤: {train_metrics['r2'] - test_metrics['r2']:.3f}")
    
    return filepath


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Ä–∞–∑–¥–µ–ª—å–Ω—ã—Ö —Ñ–∞–∑ –æ–±—É—á–µ–Ω–∏—è –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.
    """
    print("üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª—å–Ω–æ–≥–æ workflow: –û–ë–£–ß–ï–ù–ò–ï ‚Üí –°–û–•–†–ê–ù–ï–ù–ò–ï ‚Üí –ò–ù–§–ï–†–ï–ù–°")
    print("Sklearn Pipeline —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
    print("=" * 80)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    result_saver = ResultSaver()
    random_state = 42
    
    try:
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        features, target, feature_names, description = load_california_housing()
        
        # 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test (–∏–º–∏—Ç–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö)
        features_train, features_test, target_train, target_test = train_test_split(
            features, target, 
            test_size=0.2, 
            random_state=random_state,
            shuffle=True
        )
        
        print(f"\nüìã –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"   ‚Ä¢ –û–±—É—á–µ–Ω–∏–µ: {features_train.shape[0]:,} –æ–±—ä–µ–∫—Ç–æ–≤")
        print(f"   ‚Ä¢ –¢–µ—Å—Ç: {features_test.shape[0]:,} –æ–±—ä–µ–∫—Ç–æ–≤ (–∏–º–∏—Ç–∞—Ü–∏—è –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö)")
        
        # 3. –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ outlier handling
        print("\n" + "=" * 80)
        print("üî¨ –≠–¢–ê–ü 1: –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í OUTLIER HANDLING")
        print("=" * 80)
        
        best_method, comparison_results = demonstrate_different_outlier_methods(
            features_train, target_train, features_test, target_test
        )
        
        # 4. –§–ê–ó–ê –û–ë–£–ß–ï–ù–ò–Ø –° –°–û–•–†–ê–ù–ï–ù–ò–ï–ú PIPELINE
        print("\n" + "=" * 80)
        print("üéØ –≠–¢–ê–ü 2: –û–ë–£–ß–ï–ù–ò–ï –ò –°–û–•–†–ê–ù–ï–ù–ò–ï PIPELINE")
        print("=" * 80)
        
        pipeline_filepath, train_predictions, data_info = train_phase(
            features_train, target_train, best_method
        )
        
        # 5. –§–ê–ó–ê –ò–ù–§–ï–†–ï–ù–°–ê –° –ó–ê–ì–†–£–ó–ö–û–ô PIPELINE
        print("\n" + "=" * 80)
        print("üîÆ –≠–¢–ê–ü 3: –ó–ê–ì–†–£–ó–ö–ê –ò –ò–ù–§–ï–†–ï–ù–°")
        print("=" * 80)
        
        test_predictions, pipeline_metadata = inference_phase(
            features_test, pipeline_filepath
        )
        
        # 6. –ê–ù–ê–õ–ò–ó –ò –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
        print("\n" + "=" * 80)
        print("üìä –≠–¢–ê–ü 4: –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print("=" * 80)
        
        results_filepath = analyze_and_save_results(
            target_train, train_predictions,
            target_test, test_predictions,
            data_info, best_method, result_saver
        )
        
        # 7. –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê
        print(f"\n" + "=" * 80)
        print("‚úÖ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
        print("=" * 80)
        
        print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        print(f"   ‚Ä¢ Pipeline: {pipeline_filepath}")
        print(f"   ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_filepath}")
        
        print(f"\nüîÑ Workflow –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω:")
        print(f"   1. ‚úÖ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤ outlier handling")
        print(f"   2. ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ sklearn Pipeline") 
        print(f"   3. ‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ Pipeline –∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        print(f"   4. ‚úÖ –ü–æ–ª–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ JSON")
        
        print(f"\nüí° –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ:")
        print(f"   ‚Ä¢ Pipeline –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ: joblib.load('{os.path.basename(pipeline_filepath)}')")
        print(f"   ‚Ä¢ –í—Å–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥—è—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        print(f"   ‚Ä¢ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏–∑ custom_elements.py")
        
    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {str(e)}")
        raise


if __name__ == "__main__":
    main()
